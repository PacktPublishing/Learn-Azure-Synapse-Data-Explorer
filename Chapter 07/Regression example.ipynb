{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: Building Machine Learning experiments\r\n",
        "\r\n",
        "## Finding the best model with AutoML\r\n",
        "\r\n",
        "The process to use AutoML to find the best model for your machine learning task consists on four steps: loading your data, configuring your Azure Machine Learning experiment, running the experiment, and exploring the results. You will perform these tasks using your Apache Spark pool and PySpark. \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "The first code cell imports the libraries needed for this notebook:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "\n",
        "from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from notebookutils import mssparkutils\n",
        "from azureml.data.dataset_factory import TabularDatasetFactory"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define variables with the name of the linked service object that we created in the Configuring the Azure Machine Learning integration section, the name of the experiment that we created in the Finding the best model with AutoML section, and then we create a new experiment using the connection to the linked service. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linkedService_name = \"AzureML_telemetryexperiments\"\n",
        "experiment_name = \"Predict_Core_Temp\"\n",
        "\n",
        "ws = mssparkutils.azureML.getWorkspace(linkedService_name)\n",
        "experiment = Experiment(ws, experiment_name)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next code cell, you will see that the notebook that was automatically generated by the Train a new model wizard produces a SQL query that selects everything from the fleet_data table on the Lake Database. We want to change this to read from the table on the Data Explorer pool. Additionally, we will perform a few actions with our data once we load it into the Spark DataFrame:\r\n",
        "- Downsample the data: our original table has over 337,500 rows. To test this notebook and make sure everything works, it’s a good idea to work with a small subset of your data to avoid the increased costs of data retrieval, and the compute costs. This code reduces the Spark DataFrame to 1% of its size. \r\n",
        "- Select a few columns: not all columns in your table may be useful to train a model. For example, the DeviceData column, which is a JSON column that contains the device’s ID and name, is not relevant to determining the device’s core temperature. Therefore, we’re selecting here only columns that we think may help train our regression model. We will also produce three new columns that store the hour, day of month, and day of week of the telemetry event. Having this type of data separate on new columns can sometimes help find insights related to a certain fixed time or day an event happens. \r\n",
        "- Split the data in training and validation sets: when you train models, you want to separate a portion of your data to train the model and use another portion of your data to validate the model accuracy (by checking predicted data against real values). In this example, we will set 80% of the available data to the training set, and 20% to the validation set. \r\n",
        "- Create a tabular dataset from the Spark DataFrame: this is already in the notebook that was generated by the wizard, but we want to make sure we change this function to use our training set stored in the training_set variable, and not the original, full Spark DataFrame. \r\n",
        "\r\n",
        "The new code for this cell is the following: \r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the query to the Data Explorer pool table\n",
        "# Make sure you change this code to use YOUR Data Explorer pool, \n",
        "# database, and table. \n",
        "df  = spark.read \\\n",
        "    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "    .option(\"spark.synapse.linkedService\", \"kustoPool\") \\\n",
        "    .option(\"kustoCluster\", \"https://droneanalyticsadx.drone-analytics.kusto.azuresynapse.net\") \\\n",
        "    .option(\"kustoDatabase\", \"drone-telemetry\") \\\n",
        "    .option(\"kustoQuery\", \"['fleet data']\") \\\n",
        "    .load()\n",
        "\n",
        "# Downsampling data\n",
        "df = df.sample(True, 0.01, seed=1234)\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Select specific columns\n",
        "df = df.select(\n",
        "      'DeviceState', \n",
        "      'Engine1Status','Engine2Status','Engine3Status','Engine4Status',\n",
        "      'Engine1RPM','Engine2RPM','Engine3RPM','Engine4RPM',\n",
        "      'Engine1Temp','Engine2Temp','Engine3Temp','Engine4Temp',\n",
        "      'CoreTemp','BatteryTemp','CoreStatus','MemoryAvailable','BatteryLevel',\n",
        "      'Altitude','Speed','DistanceFromBase','RFSignal','PayloadWeight',\n",
        "      date_format('LocalDateTime', 'hh').alias('HourOfDay'),\n",
        "      dayofmonth('LocalDateTime').alias('DayOfMonth'),\n",
        "      dayofweek('LocalDateTime').alias('DayOfWeek')\n",
        "      )\n",
        "\n",
        "# Create the training and validation dataframes\n",
        "training_data, validation_data = df.randomSplit([0.8,0.2], seed=1234)\n",
        "\n",
        "# Create a dataset from the training dataframe, \n",
        "# using the default workspace data store\n",
        "datastore = Datastore.get_default(ws)\n",
        "dataset = TabularDatasetFactory.register_spark_dataframe(training_data, \n",
        "      datastore, name = experiment_name + \"-dataset\")"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will add a code cell to see the structure of our new Spark DataFrame and make sure everything looks good. As we can see, the new columns we created to store the hour, day of month, and day of week values are present in the Spark DataFrame. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df, summary=True)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to configure the parameters of the AutoML task. We will make a slight change to the code that was generated by the wizard: create a new Python dictionary with a series of parameters that we will then use when we create the actual AutoML configuration object. Our new settings limit the experiment’s timeout to 15 minutes, and defines other useful parameters. Here’s the code:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "automl_settings = {\n",
        "    \"iteration_timeout_minutes\": 10,\n",
        "    \"experiment_timeout_minutes\": 15,\n",
        "    \"enable_early_stopping\": True,\n",
        "    \"featurization\": 'auto',\n",
        "    \"verbosity\": logging.INFO,\n",
        "    \"n_cross_validations\": 2}\n",
        "\n",
        "automl_config = AutoMLConfig(spark_context = sc,\n",
        "                             task = 'regression',\n",
        "                             training_data = dataset,\n",
        "                             label_column_name = 'CoreTemp',\n",
        "                             primary_metric = 'r2_score',\n",
        "                             max_concurrent_iterations = 2,\n",
        "                             enable_onnx_compatible_models = False, **automl_settings)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re now finally ready to submit our AutoML experiment to Azure Machine Learning. Here, you can reuse the code generated by the wizard without any changes. This command generates a table as output, providing details about the experiment. The Details Page column offers a link to this experiment on Azure Machine Learning Studio, where you can see the status of the experiment as it runs. \r\n",
        "\r\n",
        "The code to submit the experiment is:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = experiment.submit(automl_config, show_output=True)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AutoML experiment will build a series of models with different algorithms parameters with your data. Next, we will retrieve the run that provided the best result according to the metric we specified (R2) and the model that produced this run. Once we retrieve this model, we will register it in the Azure Machine Learning model registry for future use. This code block is the same as it was created automatically for you: "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.wait_for_completion()\n",
        "\n",
        "import mlflow\n",
        "\n",
        "# Get best model from automl run\n",
        "best_run, fitted_model = run.get_output()\n",
        "\n",
        "artifact_path = experiment_name + \"_artifact\"\n",
        "\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run() as run:\n",
        "    # Save the model to the outputs directory for capture\n",
        "    mlflow.sklearn.log_model(fitted_model, artifact_path)\n",
        "\n",
        "    # Register the model to AML model registry\n",
        "    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"drone-analytics-fleet_data-20221026113234-Best\")"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "But how good is this model, really? Let’s perform a few tasks to see its accuracy. The easiest way to do this is to use real data and see if the model that we got can predict values close to the real ones. We can do this with the following code block: "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "# Create a Pandas DataFrame from the validation set\r\n",
        "validation_data_pd = validation_data.toPandas()\r\n",
        "\r\n",
        "# Create a new Pandas DataFrame with the REAL CoreTemp data\r\n",
        "df_compare = validation_data_pd.pop('CoreTemp').to_frame()\r\n",
        "\r\n",
        "# Predict the CoreTemp column using the fitted model\r\n",
        "y_predict = fitted_model.predict(validation_data_pd)\r\n",
        "\r\n",
        "# Add the predicted values to the new Pandas DataFrame\r\n",
        "# for comparison \r\n",
        "df_compare['Predicted'] = y_predict\r\n",
        "display(df_compare)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, in most cases, the predicted value is pretty close to the real value from our validation set. Those are really good results! \r\n",
        "\r\n",
        "To finalize the validation process, let’s compute the R2 score of our model, since that is the metric that we chose to evaluate it. The Y axis will show all the values that were predicted by our model, and the X axis will show all the actual values in the dataset. An optimal model should see the data points meet at the same Y and X coordinates, or as close as possible. You can use this code in a new code cell to perform this task: "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import mean_squared_error, r2_score\r\n",
        "\r\n",
        "# Compute the R2 score by using the predicted and actual CoreTemp\r\n",
        "y_test_actual = y_test['CoreTemp']\r\n",
        "r2 = r2_score(y_test_actual, y_predict)\r\n",
        "\r\n",
        "# Plot the actual versus predicted CoreTemp values\r\n",
        "plt.style.use('ggplot')\r\n",
        "plt.figure(figsize=(10, 7))\r\n",
        "plt.scatter(y_test_actual,y_predict)\r\n",
        "plt.plot([np.min(y_test_actual), np.max(y_test_actual)], [np.min(y_test_actual), np.max(y_test_actual)], color='lightblue')\r\n",
        "plt.xlabel(\"Actual\")\r\n",
        "plt.ylabel(\"Predicted\")\r\n",
        "plt.title(\"Actual vs Predicted CoreTemp R^2={}\".format(r2))\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, our model was very efficient at predicting values for our dataset, with an R2 score of 0.98.\r\n",
        "\r\n",
        "An R2 score of 1 represents a perfect model that is able to predict all occurrences perfectly, while a model that has an R2 score of 0 represents a model that is not effective. For a regression model, we want to pursue an R2 score that’s as close to 1 as possible."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = 'My automated ML model'\r\n",
        "model_path='outputs/model.pkl'\r\n",
        "model = best_run.register_model(model_name = 'DeviceTelemetry', model_path = model_path, description = description)\r\n",
        "print(model.name, model.version)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}