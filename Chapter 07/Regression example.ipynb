{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "import azureml.core\n",
        "\n",
        "from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from notebookutils import mssparkutils\n",
        "from azureml.data.dataset_factory import TabularDatasetFactory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "linkedService_name = \"AzureML_telemetryexperiments\"\n",
        "experiment_name = \"Predict_Core_Temp\"\n",
        "\n",
        "ws = mssparkutils.azureML.getWorkspace(linkedService_name)\n",
        "experiment = Experiment(ws, experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Changing the query to the Data Explorer pool table\n",
        "# Make sure you change this code to use YOUR Data Explorer pool, \n",
        "# database, and table. \n",
        "df  = spark.read \\\n",
        "    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "    .option(\"spark.synapse.linkedService\", \"kustoPool\") \\\n",
        "    .option(\"kustoCluster\", \"https://droneanalyticsadx.drone-analytics.kusto.azuresynapse.net\") \\\n",
        "    .option(\"kustoDatabase\", \"drone-telemetry\") \\\n",
        "    .option(\"kustoQuery\", \"['fleet data']\") \\\n",
        "    .load()\n",
        "\n",
        "# Downsampling data\n",
        "df = df.sample(True, 0.01, seed=1234)\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Select specific columns\n",
        "df = df.select(\n",
        "      'DeviceState', \n",
        "      'Engine1Status','Engine2Status','Engine3Status','Engine4Status',\n",
        "      'Engine1RPM','Engine2RPM','Engine3RPM','Engine4RPM',\n",
        "      'Engine1Temp','Engine2Temp','Engine3Temp','Engine4Temp',\n",
        "      'CoreTemp','BatteryTemp','CoreStatus','MemoryAvailable','BatteryLevel',\n",
        "      'Altitude','Speed','DistanceFromBase','RFSignal','PayloadWeight',\n",
        "      date_format('LocalDateTime', 'hh').alias('HourOfDay'),\n",
        "      dayofmonth('LocalDateTime').alias('DayOfMonth'),\n",
        "      dayofweek('LocalDateTime').alias('DayOfWeek')\n",
        "      )\n",
        "\n",
        "# Create the training and validation dataframes\n",
        "training_data, validation_data = df.randomSplit([0.8,0.2], seed=1234)\n",
        "\n",
        "# Create a dataset from the training dataframe, \n",
        "# using the default workspace data store\n",
        "datastore = Datastore.get_default(ws)\n",
        "dataset = TabularDatasetFactory.register_spark_dataframe(training_data, \n",
        "      datastore, name = experiment_name + \"-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df, summary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "import logging\n",
        "\n",
        "automl_settings = {\n",
        "    \"iteration_timeout_minutes\": 10,\n",
        "    \"experiment_timeout_minutes\": 15,\n",
        "    \"enable_early_stopping\": True,\n",
        "    \"featurization\": 'auto',\n",
        "    \"verbosity\": logging.INFO,\n",
        "    \"n_cross_validations\": 2}\n",
        "\n",
        "automl_config = AutoMLConfig(spark_context = sc,\n",
        "                             task = 'regression',\n",
        "                             training_data = dataset,\n",
        "                             label_column_name = 'CoreTemp',\n",
        "                             primary_metric = 'r2_score',\n",
        "                             max_concurrent_iterations = 2,\n",
        "                             enable_onnx_compatible_models = False, **automl_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "run = experiment.submit(automl_config, show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "run.wait_for_completion()\n",
        "\n",
        "import mlflow\n",
        "\n",
        "# Get best model from automl run\n",
        "best_run, fitted_model = run.get_output()\n",
        "\n",
        "artifact_path = experiment_name + \"_artifact\"\n",
        "\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run() as run:\n",
        "    # Save the model to the outputs directory for capture\n",
        "    mlflow.sklearn.log_model(fitted_model, artifact_path)\n",
        "\n",
        "    # Register the model to AML model registry\n",
        "    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"drone-analytics-fleet_data-20221026113234-Best\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "# Create a Pandas DataFrame from the validation set\r\n",
        "validation_data_pd = validation_data.toPandas()\r\n",
        "\r\n",
        "# Create a new Pandas DataFrame with the REAL CoreTemp data\r\n",
        "df_compare = validation_data_pd.pop('CoreTemp').to_frame()\r\n",
        "\r\n",
        "# Predict the CoreTemp column using the fitted model\r\n",
        "y_predict = fitted_model.predict(validation_data_pd)\r\n",
        "\r\n",
        "# Add the predicted values to the new Pandas DataFrame\r\n",
        "# for comparison \r\n",
        "df_compare['Predicted'] = y_predict\r\n",
        "display(df_compare)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import mean_squared_error, r2_score\r\n",
        "\r\n",
        "# Compute the R2 score by using the predicted and actual CoreTemp\r\n",
        "y_test_actual = y_test['CoreTemp']\r\n",
        "r2 = r2_score(y_test_actual, y_predict)\r\n",
        "\r\n",
        "# Plot the actual versus predicted CoreTemp values\r\n",
        "plt.style.use('ggplot')\r\n",
        "plt.figure(figsize=(10, 7))\r\n",
        "plt.scatter(y_test_actual,y_predict)\r\n",
        "plt.plot([np.min(y_test_actual), np.max(y_test_actual)], [np.min(y_test_actual), np.max(y_test_actual)], color='lightblue')\r\n",
        "plt.xlabel(\"Actual\")\r\n",
        "plt.ylabel(\"Predicted\")\r\n",
        "plt.title(\"Actual vs Predicted CoreTemp R^2={}\".format(r2))\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "description = 'My automated ML model'\r\n",
        "model_path='outputs/model.pkl'\r\n",
        "model = best_run.register_model(model_name = 'DeviceTelemetry', model_path = model_path, description = description)\r\n",
        "print(model.name, model.version)"
      ]
    }
  ]
}